{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlavorSaurus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pinecone import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the API keys and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "NVIDIA_API_KEY = config['API_KEYS']['NVIDIA_API_KEY']\n",
    "PINECONE_API_KEY = config['API_KEYS']['PINECONE_API_KEY']\n",
    "PINECONE_INDEX = config['PINECONE_INDEX']\n",
    "LLM_MODEL = config['LLM_MODEL']\n",
    "EMBEDDING_MODEL = config['EMBEDDING_MODEL']\n",
    "\n",
    "# Setting the NVIDIA API KEY\n",
    "os.environ[\"NVIDIA_API_KEY\"] = NVIDIA_API_KEY\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "\n",
    "# Setting the PINECONE API KEY and PINECONE INDEX\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = PINECONE_INDEX\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Loading the embedding model all-MiniLM-L6-v2\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query functions for the AI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Pinecone for similar recipes\n",
    "def query_pinecone_for_recipes(user_query):\n",
    "    # Generate the query vector from the user's input\n",
    "    query_vector = embedding_model.encode(user_query).tolist()\n",
    "    \n",
    "    # Query Pinecone for the top 5 similar vectors\n",
    "    response = index.query(vector=query_vector, top_k=5, include_metadata=True)\n",
    "    return response['matches']\n",
    "\n",
    "# Function to suggest recipes based on user query\n",
    "def suggest_recipes(user_query):\n",
    "    # Retrieve similar recipes from Pinecone\n",
    "    matches = query_pinecone_for_recipes(user_query)\n",
    "    \n",
    "    # Generate recipe suggestions using NVIDIA LLM\n",
    "    matching_recipes = []\n",
    "    for match in matches:\n",
    "        recipe_data = match['metadata']\n",
    "        recipe_prompt = f\"Recipe suggestion based on the query: '{user_query}':\\n\"\n",
    "        recipe_prompt += f\"Title: {recipe_data['title']}\\n\"\n",
    "        recipe_prompt += f\"Ingredients: {recipe_data['ingredients']}\\n\"\n",
    "        recipe_prompt += f\"Instructions: {recipe_data['instructions']}\\n\"\n",
    "        \n",
    "        matching_recipes.append(recipe_prompt)\n",
    "    \n",
    "    return matching_recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for the AI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"No need to greet everytime you reply. No need to introduce yourself everytime you reply. You are a AI Cooking Bot named \n",
    "FlavourSaurus who will give a detailed reciepe as per user's ingredients, allergies and food preferences based on {input} while taking help \n",
    "of retrived context {context} and remembering the chat history {history}. Add some dinosaur puns to the recipes. If the user's query \n",
    "is not relevant to the retrived context, come up with your own recipe.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "#LLM model used is meta/llama3-8b-instruct\n",
    "chain = prompt | ChatNVIDIA(model=LLM_MODEL) | StrOutputParser()\n",
    "\n",
    "# Query function used by the gradio UI\n",
    "def query_llm(message, history):\n",
    "    recipes = suggest_recipes(message)\n",
    "\n",
    "    context = \"\\n\".join([f\"{i+1}. {text}\" for i, text in enumerate(recipes)])\n",
    "\n",
    "    partial_message = \"\"\n",
    "\n",
    "    for chunk in chain.stream({\"input\":message, \"context\": context, \"history\": history}):\n",
    "        partial_message += chunk\n",
    "        time.sleep(0.25)\n",
    "        yield partial_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for the Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_message = \"\"\"ROAR! Ah, nice to meet you! I'm FlavourSaurus, a dino-mic cooking AI bot here to help you concoct some pre-histick-tastic recipes! When you're cooking up a storm, you can count on me to assist with ingredient suggestions, allergy-friendly options, and palate-pleasing dishes that cater to your food preferences. So, what's on your plate?\"\"\"\n",
    "\n",
    "# Gradio Interface\n",
    "gradio_interface = gr.ChatInterface(\n",
    "        query_llm,\n",
    "        chatbot=gr.Chatbot(value=[[None, default_message]]),\n",
    "        textbox=gr.Textbox(placeholder=\"Ask FlavourSaurus for food recipies as per your ingredients, allergies and food preferences!\", container=False, scale=7),\n",
    "        title=\"FlavourSaurus, a dino-mic cooking AI bot\",\n",
    "        description=f\"FlavourSaurus is a cooking AI bot who will suggest you food recipies as per your ingredients, allergies and food preferences\",\n",
    "        theme='abidlabs/Lime', # themes at https://huggingface.co/spaces/gradio/theme-gallery\n",
    "        retry_btn=None,\n",
    "        undo_btn=\"Delete Previous\",\n",
    "        clear_btn=\"Clear\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching the AI agent with Gradio UI interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following code will launch the UI on localhost (For example: Running on local URL:  http://127.0.0.1:7860)\n",
    "gradio_interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing the Gradio UI Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradio_interface.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FlavourSaurus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
